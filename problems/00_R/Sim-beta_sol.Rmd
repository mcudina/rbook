 1.
    ```{r  echo=2:20}
    set.seed(1234)
    x = rbeta(10000, shape1 = 0.5, shape2 = 0.3)
    hist(x, probability = T)
    x_pdf = seq(0, 1, by = 0.01)
    y_pdf = dbeta(x_pdf, 0.5, 0.3)
    lines(x_pdf,
          y_pdf,
          type = "l",
          col = "red",
          lwd = 2)
    ```


 2. We use Monte Carlo to estimate the probability:

    ```{r}
    x = rbeta(10000, shape1 = 0.5, shape2 = 0.3)
    y = rbeta(10000, shape1 = 0.5, shape2 = 0.3)
    z = x * y > 1 + log(y)
    mean(z)
    ```

    and the permuation test to check for (in)dependence 

    ```{r fig.height = 3, echo=2:10}
    par(mar = c(2, 2, 2, 2))
    w = x * y
    z = 1 + log(y)
    par(mfrow = c(2, 2))
    z_perm_1 = sample(z)
    z_perm_2 = sample(z)
    z_perm_3 = sample(z)
    plot(w, z)
    plot(w, z_perm_1)
    plot(w, z_perm_2)
    plot(w, z_perm_3)
    ```


    Since the first plot looks very different from the other three, we conclude that $XY$ and $1+\log(Y)$ are most likely not independent. 

 3. First, we simulate $10,000$ draws from the distribution of $M$. The function `min` is not vectorized (and it should not be!), so we cannot simply write $m = min(x,y,z)$. Luckily there is another function, called `pmin` (where `p` stands for parallel) which returns the component-wise min. Alternatively, we could use the function `apply` to apply `min` to each row of the data frame `df` which contains the simulations of $X,Y$ and $Z$:

    ```{r}
    nsim = 10000
    x = runif(nsim)
    y = runif(nsim)
    z = runif(nsim)
    m = pmin(x, y, z)
    # or, alternatively,
    df = data.frame(x, y, z)
    m = apply(df, 1, min)
    ```


    The idea is to compare the histogram of (the simulations) of $M$ to pdfs of the beta distributions for
    various values of parameters and see what fits best. The function `try_parameters` below does exacly that - it superimposes the beta pdf onto the histogram of `m`  (just like in part 1. above). We try a few different values, and finally settle on $\alpha=1$ and $\beta=3$, because it seems to fit will. It turns out that $\alpha=1$ and $\beta=3$ is, indeed, correct.
    ```{r fig.height=6, echo=2:10}
    par(mar = c(2, 4, 4, 2))
    try_parameters = function (alpha, beta) {
      hist(m,
           probability = T,
           main = paste("Trying alpha = ", alpha, " beta = ", beta))
      x_pdf = seq(0, 1, by = 0.01)
      y_pdf = dbeta(x_pdf, alpha, beta)
      lines(x_pdf,
            y_pdf,
            type = "l",
            col = "red",
            lwd = 2)
    }

    par(mfrow = c(3, 2))
    try_parameters(3, 2)
    try_parameters(2, 2)
    try_parameters(1, 0.5)
    try_parameters(0.5, 2)
    try_parameters(0.5, 3)
    try_parameters(1, 3)

    ```


    Alternatively, you could have looked up the mean and the variance of the beta distribution online, and obtained the following expressions:
    $$ \text{ mean}= \frac{\alpha}{\alpha+\beta}, \text{ variance} = \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)},$$
    and then tried to find $\alpha$ and $\beta$ to match the estimated mean and variance

    ```{r}
    c(mean(m), var(m))
    ```

    The first equation tells us that $\alpha/(\alpha+\beta)$ is about $1/4$, i.e.,  $3 \alpha \approx \beta$. We plug the obtained value of $\beta$ into the equation for variance to get the following equation:
    $$ 0.0372 \approx \frac{ 3 \alpha^2}{ (4\alpha)^2 (4 \alpha+1)^2},$$
    so that
    $$ 4 \alpha + 1 \approx \frac{3}{ 16\times 0.0372} \approx 5.04, \text{ i.e., } \alpha \approx 1 \text{ and } \beta \approx 3.$$
    This estimation technique - where the mean and variance (and possibly higher moments) are computed and matched to parameters - is called the **method of moments**.


    In case you are curious, here is how to derive the theorem from the problem (and check that our guess is, indeed, correct). We note that for any $x\in (0,1)$, $M> x$ if and only if $X>x$, $Y>x$ and $Z>x$. Therefore, by independence of $X,Y$ and $Z$, we have
    $$ \PP[ M > x ] = \PP[ X>x, Y>x, Z>x] = \PP[X>x] \times \PP[Y>x] \times  \PP[Z>x] = (1-x)^3.$$
    From there, we conclude that the cdf of $M$ is given by 
    $$F(x) = \PP[M\leq x] = 1- \PP[M>x] = 1 - (1-x)^3.$$
    Since F is a smooth function, we can differentiate it to get the pdf:
    $$ f(x) = F'(x) = 3 (1-x)^2,$$
    which is exactly the pdf of the beta distribution with parameters $3$ and $1$ (see the [Wikipedia page](https://en.wikipedia.org/wiki/Beta_distribution) of the beta distribution for this and many other facts).